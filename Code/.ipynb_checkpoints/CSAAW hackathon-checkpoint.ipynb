{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CSAAW Hackathon\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import sklearn.metrics.pairwise as Kern\n",
    "from sklearn import preprocessing \n",
    "import glob\n",
    "import os\n",
    "\n",
    "def getSigma(X,sigmaType):\n",
    "    [n,d] = X.shape\n",
    "    \n",
    "    shat = np.var(X)\n",
    "    if sigmaType == 'Scott':\n",
    "        sigma = np.mean(n ** (-1.0/(d+4) * shat))\n",
    "    elif sigmaType == 'Silverman':\n",
    "        sigma = np.mean(n ** (-1/(d+4))*shat*(4.0/(d+2.0))**(1.0/(d+4.0)))\n",
    "    \n",
    "    return sigma\n",
    "\n",
    "def getSimilarityMatrix(NormalizationFlag,SimilarityFlag):\n",
    "    dataset_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for files in glob.glob(cwd + \"/Site_specific_data/*.csv\"):\n",
    "        dataset_list.append(files)\n",
    "    \n",
    "    N = len(dataset_list)\n",
    "    dataset = genfromtxt(dataset_list[1], delimiter=',')\n",
    "    datasetTotal = np.zeros([1,dataset.shape[1]-1])\n",
    "    for site in range(0,N):\n",
    "    \n",
    "        dataset_file = dataset_list[site]\n",
    "        dataset = genfromtxt(dataset_file, delimiter=',')\n",
    "    \n",
    "        if len(dataset.shape) == 2:\n",
    "            datasetX = dataset[:,:-1]\n",
    "            datasetID = dataset[:,-1]\n",
    "        else: \n",
    "            dataset = dataset[np.newaxis,:]\n",
    "            datasetX = dataset[:,:-1]\n",
    "            datasetID = dataset[:,-1]\n",
    "    \n",
    "        if NormalizationFlag == 0:\n",
    "            dataset_normalized = preprocessing.normalize(datasetX, norm='l2')\n",
    "        elif NormalizationFlag == 1:\n",
    "            dataset_normalized = preprocessing.scale(datasetX)\n",
    "        elif NormalizationFlag == 2:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset_normalized = min_max_scaler.fit_transform(datasetX)\n",
    "        \n",
    "        dataset_site = 'dataset' + str(site)\n",
    "        exec (dataset_site + \" = dataset_normalized\")\n",
    "    \n",
    "        datasetTotal = np.concatenate((datasetTotal,dataset_normalized),axis = 0)\n",
    "    \n",
    "    datasetTotal = datasetTotal[1:,:]\n",
    "    bw_prob = getSigma(datasetTotal,'Scott')\n",
    "    print bw_prob\n",
    "    Sim = np.zeros([N,N])\n",
    "    for ii in range(0,N):\n",
    "        dataset_ii = 'dataset' + str(ii)\n",
    "        X_ii = eval(dataset_ii)\n",
    "        for jj in range(0,N):\n",
    "            dataset_jj = 'dataset' + str(jj)\n",
    "            X_jj = eval(dataset_jj)\n",
    "            if SimilarityFlag == 0:\n",
    "                SimWithinSite = euclidean_distances(X_ii,X_jj) \n",
    "                Sim[ii,jj] = np.linalg.norm(SimWithinSite,ord = 'fro')/(float(M.shape[0])*float(M.shape[1]))\n",
    "            elif SimilarityFlag == 1:\n",
    "                SimWithinSite = Kern.rbf_kernel(X_ii,X_jj,bw_prob)\n",
    "                Sim[ii,jj] = np.mean(SimWithinSite)\n",
    "\n",
    "    if SimilarityFlag == 2:\n",
    "        SimFinal = np.zeros([N,N])\n",
    "        for ii in range(0,N):\n",
    "            dataset_ii = 'dataset' + str(ii)\n",
    "            for jj in range(0,N):\n",
    "                dataset_jj = 'dataset' + str(jj)\n",
    "                if SimilarityFlag == 0:\n",
    "                    SimFinal[ii,jj] = Sim[ii,jj]\n",
    "                elif SimilarityFlag == 1:\n",
    "                    expVar = Sim[i,i] + Sim[j,j] - 2*Sim[i,j]\n",
    "                    SimFinal[ii,jj] = np.exp(-sim*bwFinal)\n",
    "        Sim = np.copy(SimFinal)\n",
    "    return Sim, datasetTotal\n",
    "\n",
    "\n",
    "def getSimilarityMatrixEra(NormalizationFlag,SimilarityFlag):\n",
    "    dataset_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for files in glob.glob(cwd + \"/Site_specific_data_with_ERA/*.csv\"):\n",
    "        dataset_list.append(files)\n",
    "    \n",
    "    N = len(dataset_list)\n",
    "    dataset = genfromtxt(dataset_list[1], delimiter=',')\n",
    "    datasetTotal = np.zeros([1,dataset.shape[1]-2])\n",
    "    for site in range(0,N):\n",
    "    \n",
    "        dataset_file = dataset_list[site]\n",
    "        dataset = genfromtxt(dataset_file, delimiter=',')\n",
    "    \n",
    "        if len(dataset.shape) == 2:\n",
    "            datasetX = dataset[:,:-2]\n",
    "            datasetERA = dataset[:,-1]\n",
    "            datasetID = dataset[:,-2]\n",
    "        else: \n",
    "            dataset = dataset[np.newaxis,:]\n",
    "            datasetX = dataset[:,:-2]\n",
    "            datasetERA = dataset[:,-1]\n",
    "            datasetID = dataset[:,-2]\n",
    "    \n",
    "        if NormalizationFlag == 0:\n",
    "            dataset_normalized = preprocessing.normalize(datasetX, norm='l2')\n",
    "        elif NormalizationFlag == 1:\n",
    "            dataset_normalized = preprocessing.scale(datasetX)\n",
    "        elif NormalizationFlag == 2:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset_normalized = min_max_scaler.fit_transform(datasetX)\n",
    "        \n",
    "        dataset_site = 'dataset' + str(site)\n",
    "        exec (dataset_site + \" = dataset_normalized\")\n",
    "        \n",
    "        dataset_site_era = 'dataset_era' + str(site)\n",
    "        exec (dataset_site_era + \" = datasetERA\" )\n",
    "    \n",
    "        datasetTotal = np.concatenate((datasetTotal,dataset_normalized),axis = 0)\n",
    "    \n",
    "    datasetTotal = datasetTotal[1:,:]\n",
    "    bw_prob = getSigma(datasetTotal,'Scott')\n",
    "    print bw_prob\n",
    "    Sim = np.zeros([N,N])\n",
    "    for ii in range(0,N):\n",
    "        dataset_ii = 'dataset' + str(ii)\n",
    "        X_iiEra = eval('dataset_era' + str(ii))\n",
    "        X_ii = eval(dataset_ii)\n",
    "        for jj in range(0,N):\n",
    "            dataset_jj = 'dataset' + str(jj)\n",
    "            X_jjEra = eval('dataset_era' + str(jj))\n",
    "            X_jj = eval(dataset_jj)\n",
    "            if SimilarityFlag == 0:\n",
    "                SimWithinSite = euclidean_distances(X_ii,X_jj) \n",
    "                Sim[ii,jj] = np.linalg.norm(SimWithinSite,ord = 'fro')/(float(M.shape[0])*float(M.shape[1]))\n",
    "            elif SimilarityFlag == 1:\n",
    "                SimWithinSite = Kern.rbf_kernel(X_ii,X_jj,bw_prob)\n",
    "                EraSim = np.zeros([X_ii.shape[0],X_jj.shape[0]])\n",
    "                for pp in range(0,X_ii.shape[0]):\n",
    "                    for qq in range(0,X_jj.shape[0]):\n",
    "                        EraSim[pp,qq] = np.sign(X_iiEra[pp] - X_jjEra[qq])\n",
    "                \n",
    "                \n",
    "                Sim[ii,jj] = np.mean(SimWithinSite*EraSim)\n",
    "    return Sim\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805326289851\n",
      "(822, 27)\n",
      "0.808060984097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#n = 100#datasetTotal.shape[0]\\n#inertia = np.zeros([n])\\n#for i in range(0,n):\\n#    centroids,labels,inertia[i] = cluster.k_means(datasetTotal,n_clusters=31)\\n#plt.plot(range(0,n), inertia)\\n#plt.ylabel('Distances')\\n#plt.show()\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "thres=0.04\n",
    "matrix_type='dist'\n",
    "\n",
    "NormalizationFlag = 1 # 0 is normalizing by norm, 1 is scale it, 2 is make it in between [0,1]\n",
    "SimilarityFlag = 1 #0 if euclidean DISTANCE (note distance and similarity), 1 if RKHS (mean embedding) with linear kernel, 2 if RKHS with Gaussian kernel\n",
    "Sim, datasetTotal = getSimilarityMatrix(NormalizationFlag,SimilarityFlag)\n",
    "\n",
    "print datasetTotal.shape\n",
    "SimEra = getSimilarityMatrixEra(NormalizationFlag,SimilarityFlag)\n",
    "\n",
    "'''\n",
    "#n = 100#datasetTotal.shape[0]\n",
    "#inertia = np.zeros([n])\n",
    "#for i in range(0,n):\n",
    "#    centroids,labels,inertia[i] = cluster.k_means(datasetTotal,n_clusters=31)\n",
    "#plt.plot(range(0,n), inertia)\n",
    "#plt.ylabel('Distances')\n",
    "#plt.show()\n",
    "'''\n",
    "\n",
    "#centroids,labels,inertia = cluster.k_means(datasetTotal,n_clusters=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Sim1 = np.log(Sim)\n",
    "X = Sim1.ravel()\n",
    "n, bins, patches = plt.hist(X, 100, facecolor='g', alpha=0.75)\n",
    "\n",
    "plt.title('Histogram of sim')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803579418463\n",
      "[[  2.50000005e-01   2.64683179e-05   1.06908283e-05   6.46980913e-07\n",
      "    1.72651630e-09   4.47441993e-05   2.63184488e-04   9.55082263e-13\n",
      "    2.33830055e-04   6.00182413e-07   2.30477169e-11   4.02315279e-05\n",
      "    2.79093361e-08   4.98238143e-04   6.19558063e-06   4.00245570e-07\n",
      "    9.99688487e-07   3.75153241e-06   1.06075954e-06   1.13003806e-05\n",
      "    6.08664063e-05   1.25871027e-06   1.83121214e-06   3.40987437e-07\n",
      "    6.71160677e-06   1.09377955e-05   7.19291589e-07   8.21039443e-07\n",
      "    2.95873012e-05   1.29429455e-08   3.30149590e-06]\n",
      " [  2.64683179e-05   2.29990924e-02   1.22044471e-05   8.54450946e-06\n",
      "    1.64187619e-06   1.71503949e-04   2.76869312e-04   1.13392265e-09\n",
      "    3.71470747e-04   2.82361851e-06   1.29056551e-08   4.01528299e-04\n",
      "    3.66954337e-08   3.76304362e-03   1.67054571e-05   8.72327111e-06\n",
      "    7.85022978e-06   3.69567845e-06   1.21307025e-05   1.91765346e-05\n",
      "    7.63841850e-05   1.81041106e-05   2.02862637e-05   6.13677294e-07\n",
      "    3.89720004e-05   9.50221191e-05   2.25075158e-05   4.90806053e-06\n",
      "    7.63488825e-05   1.31423989e-06   2.35215133e-05]\n",
      " [  1.06908283e-05   1.22044471e-05   2.50000004e-01   4.63875349e-08\n",
      "    1.33487418e-10   2.60316918e-06   8.18033163e-06   3.29759512e-14\n",
      "    2.29251780e-05   2.28600676e-07   4.13188696e-11   1.10060999e-04\n",
      "    5.73778727e-11   7.07721386e-06   7.50496069e-07   1.07335994e-07\n",
      "    3.71698607e-07   6.70271959e-07   1.71732416e-07   1.33755201e-05\n",
      "    1.10214422e-07   8.99113209e-08   4.03134555e-07   3.11954964e-07\n",
      "    9.91080313e-07   1.27132343e-05   5.51194665e-07   1.35293631e-07\n",
      "    1.41347788e-06   3.90525302e-08   9.94288432e-08]\n",
      " [  6.46980913e-07   8.54450946e-06   4.63875349e-08   2.30458207e-02\n",
      "    1.77715659e-07   1.99791426e-06   3.65717031e-05   2.90835862e-10\n",
      "    3.32099695e-05   3.12756185e-06   2.01541792e-09   2.90018982e-05\n",
      "    9.24678871e-08   5.31846970e-04   7.03701372e-05   4.76088863e-06\n",
      "    9.14319693e-05   2.39686827e-05   5.92680805e-05   2.36912500e-05\n",
      "    6.01737752e-05   5.15585664e-06   1.80207842e-05   6.95881226e-07\n",
      "    3.04206060e-05   2.89560880e-05   3.26717697e-06   2.05473571e-06\n",
      "    8.82473422e-06   8.27744965e-06   7.65474629e-06]\n",
      " [  1.72651630e-09   1.64187619e-06   1.33487418e-10   1.77715659e-07\n",
      "    3.33333333e-01   3.33762658e-08   3.06432868e-08   3.06729695e-14\n",
      "    6.66848336e-08   4.37545176e-08   3.12360687e-11   1.88494641e-10\n",
      "    1.82171449e-10   2.66105971e-06   9.49211662e-08   3.52740594e-08\n",
      "    3.64105926e-08   4.88930719e-08   1.12482480e-06   2.26955854e-08\n",
      "    3.64504056e-06   5.58938881e-09   1.77453594e-08   1.43255739e-11\n",
      "    1.07171589e-06   4.33769092e-07   3.44411475e-12   2.80986256e-09\n",
      "    2.00673479e-08   2.42059087e-09   7.82824222e-07]\n",
      " [  4.47441993e-05   1.71503949e-04   2.60316918e-06   1.99791426e-06\n",
      "    3.33762658e-08   2.67444888e-02   8.64169431e-04   1.47918463e-10\n",
      "    5.77032870e-04   2.84965097e-06   4.95519027e-10   2.55826736e-04\n",
      "    5.95412670e-09   1.01077970e-03   5.84404196e-06   3.28029211e-06\n",
      "    9.07354135e-06   1.37436782e-06   1.90220334e-05   2.84293732e-05\n",
      "    5.26103232e-06   1.21875062e-05   2.91489644e-05   2.23176790e-06\n",
      "    6.88603655e-06   2.46709022e-05   2.78864150e-04   5.73284656e-07\n",
      "    7.94035670e-05   2.56420799e-06   9.12419846e-06]\n",
      " [  2.63184488e-04   2.76869312e-04   8.18033163e-06   3.65717031e-05\n",
      "    3.06432868e-08   8.64169431e-04   3.29948682e-02   1.27499644e-10\n",
      "    3.46839609e-03   3.44927181e-05   1.86898191e-09   1.31433883e-03\n",
      "    1.04947754e-06   1.16759763e-02   5.52200670e-05   9.50993521e-06\n",
      "    3.11058079e-04   1.01052811e-05   1.50696055e-05   1.51744731e-04\n",
      "    1.04566450e-05   2.86001535e-05   5.24877191e-04   1.00283964e-05\n",
      "    7.45980752e-05   1.46549567e-04   1.57975762e-04   3.48905162e-06\n",
      "    2.80912550e-04   7.69768827e-06   8.87617179e-06]\n",
      " [  9.55082263e-13   1.13392265e-09   3.29759512e-14   2.90835862e-10\n",
      "    3.06729695e-14   1.47918463e-10   1.27499644e-10   5.00000000e-01\n",
      "    5.30466237e-10   3.37793209e-12   2.83401664e-14   3.01286302e-10\n",
      "    5.22893459e-13   3.77804482e-10   8.22240436e-10   2.44998667e-12\n",
      "    5.49849051e-09   1.73186918e-11   1.03205165e-11   1.81065150e-11\n",
      "    6.99600145e-11   4.10115756e-10   1.50946101e-10   4.02408535e-10\n",
      "    3.42981513e-11   1.02422012e-11   1.33222043e-11   4.20618240e-11\n",
      "    4.64762614e-10   3.10769342e-09   4.53254563e-09]\n",
      " [  2.33830055e-04   3.71470747e-04   2.29251780e-05   3.32099695e-05\n",
      "    6.66848336e-08   5.77032870e-04   3.46839609e-03   5.30466237e-10\n",
      "    2.57706402e-02   2.12668149e-05   3.94714393e-09   1.28201808e-03\n",
      "    4.81673750e-07   1.24656140e-02   5.56366164e-05   1.27004609e-05\n",
      "    4.45202378e-05   2.89223465e-05   5.81246901e-05   8.05843053e-05\n",
      "    8.73130462e-05   1.22301976e-04   8.04225948e-05   3.59470359e-05\n",
      "    9.95687194e-05   1.05396482e-04   2.99018858e-05   6.07047808e-06\n",
      "    1.73773024e-04   1.09780469e-05   2.20180607e-05]\n",
      " [  6.00182413e-07   2.82361851e-06   2.28600676e-07   3.12756185e-06\n",
      "    4.37545176e-08   2.84965097e-06   3.44927181e-05   3.37793209e-12\n",
      "    2.12668149e-05   4.35171375e-02   1.27797541e-09   3.49757635e-05\n",
      "    6.23790695e-08   7.10539069e-05   8.97813255e-06   1.85135185e-06\n",
      "    7.91027794e-06   8.31119330e-06   4.40571365e-06   2.81332951e-06\n",
      "    8.97761816e-06   2.73059967e-06   1.72564949e-05   4.98819423e-06\n",
      "    1.60627219e-05   6.21959475e-06   1.17054733e-04   6.55895701e-07\n",
      "    1.38570904e-05   6.73429880e-07   3.51981325e-06]\n",
      " [  2.30477169e-11   1.29056551e-08   4.13188696e-11   2.01541792e-09\n",
      "    3.12360687e-11   4.95519027e-10   1.86898191e-09   2.83401664e-14\n",
      "    3.94714393e-09   1.27797541e-09   2.50000000e-01   1.05641974e-10\n",
      "    3.48911217e-12   7.51269731e-08   2.04918831e-08   5.78112828e-11\n",
      "    5.02175603e-12   6.12887666e-10   8.40460528e-10   3.43175022e-08\n",
      "    1.21372127e-08   3.26229008e-09   9.98734489e-09   6.41194482e-11\n",
      "    2.24654261e-09   1.83157617e-09   1.55391326e-10   6.15938528e-09\n",
      "    5.93690813e-09   1.89980385e-11   1.34258798e-09]\n",
      " [  4.02315279e-05   4.01528299e-04   1.10060999e-04   2.90018982e-05\n",
      "    1.88494641e-10   2.55826736e-04   1.31433883e-03   3.01286302e-10\n",
      "    1.28201808e-03   3.49757635e-05   1.05641974e-10   1.27216206e-01\n",
      "    8.00155984e-08   1.84819066e-03   2.37689837e-05   3.17064871e-06\n",
      "    9.90333497e-05   7.25698968e-05   2.17131117e-06   4.52879846e-05\n",
      "    1.95278536e-06   9.05800565e-06   1.67956034e-05   2.38109689e-07\n",
      "    2.37344983e-05   9.88396396e-05   6.68499613e-07   2.88012718e-06\n",
      "    1.94179633e-04   2.49618069e-06   4.12037297e-06]\n",
      " [  2.79093361e-08   3.66954337e-08   5.73778727e-11   9.24678871e-08\n",
      "    1.82171449e-10   5.95412670e-09   1.04947754e-06   5.22893459e-13\n",
      "    4.81673750e-07   6.23790695e-08   3.48911217e-12   8.00155984e-08\n",
      "    3.33333333e-01   3.47756941e-06   3.23433151e-07   3.67642234e-08\n",
      "    1.11067401e-09   2.13821313e-07   1.96562247e-06   3.23803126e-07\n",
      "    3.62165406e-08   7.37962330e-09   1.07802935e-08   4.59204449e-09\n",
      "    8.80526588e-07   1.40195889e-07   7.53719751e-09   2.62669234e-08\n",
      "    5.38044541e-08   2.75404490e-08   2.50794728e-06]\n",
      " [  4.98238143e-04   3.76304362e-03   7.07721386e-06   5.31846970e-04\n",
      "    2.66105971e-06   1.01077970e-03   1.16759763e-02   3.77804482e-10\n",
      "    1.24656140e-02   7.10539069e-05   7.51269731e-08   1.84819066e-03\n",
      "    3.47756941e-06   1.00000000e+00   2.11969876e-03   5.89033322e-05\n",
      "    2.11967554e-04   2.41548876e-05   3.14527781e-04   2.39705475e-04\n",
      "    1.74933455e-03   2.30229403e-04   2.27990878e-03   7.92722323e-06\n",
      "    2.98668415e-03   7.71660762e-04   8.08396293e-05   8.39985657e-05\n",
      "    2.09056632e-03   7.79796860e-05   1.06217867e-03]\n",
      " [  6.19558063e-06   1.67054571e-05   7.50496069e-07   7.03701372e-05\n",
      "    9.49211662e-08   5.84404196e-06   5.52200670e-05   8.22240436e-10\n",
      "    5.56366164e-05   8.97813255e-06   2.04918831e-08   2.37689837e-05\n",
      "    3.23433151e-07   2.11969876e-03   1.09942297e-02   1.08229518e-05\n",
      "    7.70331806e-06   1.08065008e-05   2.97183742e-05   2.12074203e-05\n",
      "    5.88585822e-05   1.99573665e-05   1.52349044e-04   5.53247227e-06\n",
      "    3.23374301e-05   1.74217331e-05   5.11939216e-06   2.96523963e-06\n",
      "    2.25082234e-05   1.49225028e-05   1.65931115e-05]\n",
      " [  4.00245570e-07   8.72327111e-06   1.07335994e-07   4.76088863e-06\n",
      "    3.52740594e-08   3.28029211e-06   9.50993521e-06   2.44998667e-12\n",
      "    1.27004609e-05   1.85135185e-06   5.78112828e-11   3.17064871e-06\n",
      "    3.67642234e-08   5.89033322e-05   1.08229518e-05   5.00209671e-02\n",
      "    4.61010113e-05   4.08745470e-07   4.65105210e-06   1.37740173e-06\n",
      "    8.04498146e-06   4.54505618e-06   2.19893732e-07   2.41593636e-07\n",
      "    1.65526455e-06   8.88689994e-06   6.93308670e-07   4.35884359e-08\n",
      "    4.09926906e-06   3.10996453e-07   2.51302553e-06]\n",
      " [  9.99688487e-07   7.85022978e-06   3.71698607e-07   9.14319693e-05\n",
      "    3.64105926e-08   9.07354135e-06   3.11058079e-04   5.49849051e-09\n",
      "    4.45202378e-05   7.91027794e-06   5.02175603e-12   9.90333497e-05\n",
      "    1.11067401e-09   2.11967554e-04   7.70331806e-06   4.61010113e-05\n",
      "    8.33352333e-02   1.59807679e-04   4.95615079e-06   9.83829166e-06\n",
      "    5.00551925e-06   1.06096337e-06   1.49513733e-05   3.97209699e-06\n",
      "    9.60736753e-06   7.13465913e-06   9.93663935e-08   8.85415682e-07\n",
      "    7.89702498e-06   2.97116956e-06   3.28062503e-06]\n",
      " [  3.75153241e-06   3.69567845e-06   6.70271959e-07   2.39686827e-05\n",
      "    4.88930719e-08   1.37436782e-06   1.01052811e-05   1.73186918e-11\n",
      "    2.89223465e-05   8.31119330e-06   6.12887666e-10   7.25698968e-05\n",
      "    2.13821313e-07   2.41548876e-05   1.08065008e-05   4.08745470e-07\n",
      "    1.59807679e-04   5.02253518e-02   5.46457148e-06   3.41683944e-06\n",
      "    9.23250377e-05   3.07483082e-06   1.55984629e-06   1.21041657e-07\n",
      "    1.32550709e-05   4.74778577e-06   5.83408191e-09   2.01435696e-06\n",
      "    2.16601229e-06   6.52941373e-07   1.38015167e-06]\n",
      " [  1.06075954e-06   1.21307025e-05   1.71732416e-07   5.92680805e-05\n",
      "    1.12482480e-06   1.90220334e-05   1.50696055e-05   1.03205165e-11\n",
      "    5.81246901e-05   4.40571365e-06   8.40460528e-10   2.17131117e-06\n",
      "    1.96562247e-06   3.14527781e-04   2.97183742e-05   4.65105210e-06\n",
      "    4.95615079e-06   5.46457148e-06   4.02521574e-02   2.89512883e-05\n",
      "    1.32130039e-04   1.57223615e-06   7.74699174e-06   5.13013300e-06\n",
      "    3.27203540e-05   2.57651502e-05   1.03520694e-05   1.28359966e-05\n",
      "    1.19978061e-05   4.98254054e-06   1.66486986e-05]\n",
      " [  1.13003806e-05   1.91765346e-05   1.33755201e-05   2.36912500e-05\n",
      "    2.26955854e-08   2.84293732e-05   1.51744731e-04   1.81065150e-11\n",
      "    8.05843053e-05   2.81332951e-06   3.43175022e-08   4.52879846e-05\n",
      "    3.23803126e-07   2.39705475e-04   2.12074203e-05   1.37740173e-06\n",
      "    9.83829166e-06   3.41683944e-06   2.89512883e-05   2.09802561e-02\n",
      "    3.12732785e-05   1.07574099e-05   2.20292008e-05   5.84571150e-05\n",
      "    3.78884810e-05   9.95164142e-06   1.13620635e-05   7.45471209e-06\n",
      "    2.34870836e-05   1.33996642e-06   8.62261946e-06]\n",
      " [  6.08664063e-05   7.63841850e-05   1.10214422e-07   6.01737752e-05\n",
      "    3.64504056e-06   5.26103232e-06   1.04566450e-05   6.99600145e-11\n",
      "    8.73130462e-05   8.97761816e-06   1.21372127e-08   1.95278536e-06\n",
      "    3.62165406e-08   1.74933455e-03   5.88585822e-05   8.04498146e-06\n",
      "    5.00551925e-06   9.23250377e-05   1.32130039e-04   3.12732785e-05\n",
      "    5.01331275e-02   7.84916366e-06   1.39634821e-05   9.90087492e-07\n",
      "    8.26202984e-05   6.11648682e-05   1.72669470e-06   8.81647220e-06\n",
      "    1.90299784e-05   1.26376607e-06   1.56497654e-05]\n",
      " [  1.25871027e-06   1.81041106e-05   8.99113209e-08   5.15585664e-06\n",
      "    5.58938881e-09   1.21875062e-05   2.86001535e-05   4.10115756e-10\n",
      "    1.22301976e-04   2.73059967e-06   3.26229008e-09   9.05800565e-06\n",
      "    7.37962330e-09   2.30229403e-04   1.99573665e-05   4.54505618e-06\n",
      "    1.06096337e-06   3.07483082e-06   1.57223615e-06   1.07574099e-05\n",
      "    7.84916366e-06   5.05111894e-02   7.45617465e-06   1.82238704e-06\n",
      "    7.29701667e-06   5.36454240e-06   5.33516688e-07   7.11607382e-07\n",
      "    1.32598012e-05   3.89384163e-07   1.65945339e-06]\n",
      " [  1.83121214e-06   2.02862637e-05   4.03134555e-07   1.80207842e-05\n",
      "    1.77453594e-08   2.91489644e-05   5.24877191e-04   1.50946101e-10\n",
      "    8.04225948e-05   1.72564949e-05   9.98734489e-09   1.67956034e-05\n",
      "    1.07802935e-08   2.27990878e-03   1.52349044e-04   2.19893732e-07\n",
      "    1.49513733e-05   1.55984629e-06   7.74699174e-06   2.20292008e-05\n",
      "    1.39634821e-05   7.45617465e-06   3.37380528e-02   1.80759499e-06\n",
      "    2.86134520e-05   8.03406673e-06   3.91408953e-05   1.61307190e-06\n",
      "    2.85390922e-05   3.36868945e-06   3.64087262e-06]\n",
      " [  3.40987437e-07   6.13677294e-07   3.11954964e-07   6.95881226e-07\n",
      "    1.43255739e-11   2.23176790e-06   1.00283964e-05   4.02408535e-10\n",
      "    3.59470359e-05   4.98819423e-06   6.41194482e-11   2.38109689e-07\n",
      "    4.59204449e-09   7.92722323e-06   5.53247227e-06   2.41593636e-07\n",
      "    3.97209699e-06   1.21041657e-07   5.13013300e-06   5.84571150e-05\n",
      "    9.90087492e-07   1.82238704e-06   1.80759499e-06   1.25000196e-01\n",
      "    1.64496961e-06   4.22332556e-07   5.92492562e-08   1.75077931e-08\n",
      "    1.84621937e-05   1.21002303e-06   6.52509461e-08]\n",
      " [  6.71160677e-06   3.89720004e-05   9.91080313e-07   3.04206060e-05\n",
      "    1.07171589e-06   6.88603655e-06   7.45980752e-05   3.42981513e-11\n",
      "    9.95687194e-05   1.60627219e-05   2.24654261e-09   2.37344983e-05\n",
      "    8.80526588e-07   2.98668415e-03   3.23374301e-05   1.65526455e-06\n",
      "    9.60736753e-06   1.32550709e-05   3.27203540e-05   3.78884810e-05\n",
      "    8.26202984e-05   7.29701667e-06   2.86134520e-05   1.64496961e-06\n",
      "    1.71195207e-02   2.50088684e-05   3.69859477e-06   1.58619799e-06\n",
      "    1.35408575e-05   2.49345478e-06   3.04785605e-05]\n",
      " [  1.09377955e-05   9.50221191e-05   1.27132343e-05   2.89560880e-05\n",
      "    4.33769092e-07   2.46709022e-05   1.46549567e-04   1.02422012e-11\n",
      "    1.05396482e-04   6.21959475e-06   1.83157617e-09   9.88396396e-05\n",
      "    1.40195889e-07   7.71660762e-04   1.74217331e-05   8.88689994e-06\n",
      "    7.13465913e-06   4.74778577e-06   2.57651502e-05   9.95164142e-06\n",
      "    6.11648682e-05   5.36454240e-06   8.03406673e-06   4.22332556e-07\n",
      "    2.50088684e-05   1.96984319e-02   1.27063001e-05   1.31680478e-06\n",
      "    9.35399011e-06   4.32867104e-06   2.62147697e-05]\n",
      " [  7.19291589e-07   2.25075158e-05   5.51194665e-07   3.26717697e-06\n",
      "    3.44411475e-12   2.78864150e-04   1.57975762e-04   1.33222043e-11\n",
      "    2.99018858e-05   1.17054733e-04   1.55391326e-10   6.68499613e-07\n",
      "    7.53719751e-09   8.08396293e-05   5.11939216e-06   6.93308670e-07\n",
      "    9.93663935e-08   5.83408191e-09   1.03520694e-05   1.13620635e-05\n",
      "    1.72669470e-06   5.33516688e-07   3.91408953e-05   5.92492562e-08\n",
      "    3.69859477e-06   1.27063001e-05   3.33333333e-01   1.34864397e-08\n",
      "    1.68265448e-05   1.18402619e-07   5.60580606e-06]\n",
      " [  8.21039443e-07   4.90806053e-06   1.35293631e-07   2.05473571e-06\n",
      "    2.80986256e-09   5.73284656e-07   3.48905162e-06   4.20618240e-11\n",
      "    6.07047808e-06   6.55895701e-07   6.15938528e-09   2.88012718e-06\n",
      "    2.62669234e-08   8.39985657e-05   2.96523963e-06   4.35884359e-08\n",
      "    8.85415682e-07   2.01435696e-06   1.28359966e-05   7.45471209e-06\n",
      "    8.81647220e-06   7.11607382e-07   1.61307190e-06   1.75077931e-08\n",
      "    1.58619799e-06   1.31680478e-06   1.34864397e-08   5.55557524e-02\n",
      "    4.93564356e-06   2.98021274e-07   3.21885315e-06]\n",
      " [  2.95873012e-05   7.63488825e-05   1.41347788e-06   8.82473422e-06\n",
      "    2.00673479e-08   7.94035670e-05   2.80912550e-04   4.64762614e-10\n",
      "    1.73773024e-04   1.38570904e-05   5.93690813e-09   1.94179633e-04\n",
      "    5.38044541e-08   2.09056632e-03   2.25082234e-05   4.09926906e-06\n",
      "    7.89702498e-06   2.16601229e-06   1.19978061e-05   2.34870836e-05\n",
      "    1.90299784e-05   1.32598012e-05   2.85390922e-05   1.84621937e-05\n",
      "    1.35408575e-05   9.35399011e-06   1.68265448e-05   4.93564356e-06\n",
      "    2.50345157e-02   2.20620453e-06   1.66224809e-05]\n",
      " [  1.29429455e-08   1.31423989e-06   3.90525302e-08   8.27744965e-06\n",
      "    2.42059087e-09   2.56420799e-06   7.69768827e-06   3.10769342e-09\n",
      "    1.09780469e-05   6.73429880e-07   1.89980385e-11   2.49618069e-06\n",
      "    2.75404490e-08   7.79796860e-05   1.49225028e-05   3.10996453e-07\n",
      "    2.97116956e-06   6.52941373e-07   4.98254054e-06   1.33996642e-06\n",
      "    1.26376607e-06   3.89384163e-07   3.36868945e-06   1.21002303e-06\n",
      "    2.49345478e-06   4.32867104e-06   1.18402619e-07   2.98021274e-07\n",
      "    2.20620453e-06   5.00046920e-02   9.80007936e-07]\n",
      " [  3.30149590e-06   2.35215133e-05   9.94288432e-08   7.65474629e-06\n",
      "    7.82824222e-07   9.12419846e-06   8.87617179e-06   4.53254563e-09\n",
      "    2.20180607e-05   3.51981325e-06   1.34258798e-09   4.12037297e-06\n",
      "    2.50794728e-06   1.06217867e-03   1.65931115e-05   2.51302553e-06\n",
      "    3.28062503e-06   1.38015167e-06   1.66486986e-05   8.62261946e-06\n",
      "    1.56497654e-05   1.65945339e-06   3.64087262e-06   6.52509461e-08\n",
      "    3.04785605e-05   2.62147697e-05   5.60580606e-06   3.21885315e-06\n",
      "    1.66224809e-05   9.80007936e-07   2.22602631e-02]]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named py",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-fa7e2725ffb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mSimilaritMatrixCreation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named py"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46691177  0.68346885]\n",
      " [ 0.46024211  0.39136154]\n",
      " [ 0.24173493  0.21591235]\n",
      " [ 0.55547421  0.23669852]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn.metrics.pairwise as Kern\n",
    "\n",
    "X = np.random.rand(4,2)\n",
    "X1 = np.random.rand(4,2)\n",
    "A = Kern.rbf_kernel(X,X1,0.05)\n",
    "                    \n",
    "print X*X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CSAAW Hackathon\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import sklearn.metrics.pairwise as Kern\n",
    "from sklearn import preprocessing \n",
    "import glob\n",
    "import os\n",
    "\n",
    "def getSigma(X,sigmaType):\n",
    "    [n,d] = X.shape\n",
    "    \n",
    "    shat = np.var(X)\n",
    "    if sigmaType == 'Scott':\n",
    "        sigma = np.mean(n ** (-1.0/(d+4) * shat))\n",
    "    elif sigmaType == 'Silverman':\n",
    "        sigma = np.mean(n ** (-1/(d+4))*shat*(4.0/(d+2.0))**(1.0/(d+4.0)))\n",
    "    \n",
    "    return sigma\n",
    "\n",
    "def getSimilarityMatrix(NormalizationFlag,SimilarityFlag,filepath):\n",
    "    dataset_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for files in glob.glob(cwd +filepath+ \"*.csv\"):\n",
    "        dataset_list.append(files)\n",
    "    \n",
    "    N = len(dataset_list)\n",
    "    dataset = genfromtxt(dataset_list[1], delimiter=',')\n",
    "    datasetTotal = np.zeros([1,dataset.shape[1]-1])\n",
    "    for site in range(0,N):\n",
    "    \n",
    "        dataset_file = dataset_list[site]\n",
    "        dataset = genfromtxt(dataset_file, delimiter=',')\n",
    "    \n",
    "        if len(dataset.shape) == 2:\n",
    "            datasetX = dataset[:,:-1]\n",
    "            datasetID = dataset[:,-1]\n",
    "        else: \n",
    "            dataset = dataset[np.newaxis,:]\n",
    "            datasetX = dataset[:,:-1]\n",
    "            datasetID = dataset[:,-1]\n",
    "    \n",
    "        if NormalizationFlag == 0:\n",
    "            dataset_normalized = preprocessing.normalize(datasetX, norm='l2')\n",
    "        elif NormalizationFlag == 1:\n",
    "            dataset_normalized = preprocessing.scale(datasetX)\n",
    "        elif NormalizationFlag == 2:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset_normalized = min_max_scaler.fit_transform(datasetX)\n",
    "        \n",
    "        dataset_site = 'dataset' + str(site)\n",
    "        exec (dataset_site + \" = dataset_normalized\")\n",
    "    \n",
    "        datasetTotal = np.concatenate((datasetTotal,dataset_normalized),axis = 0)\n",
    "    \n",
    "    datasetTotal = datasetTotal[1:,:]\n",
    "    bw_prob = getSigma(datasetTotal,'Scott')\n",
    "    print bw_prob\n",
    "    Sim = np.zeros([N,N])\n",
    "    for ii in range(0,N):\n",
    "        dataset_ii = 'dataset' + str(ii)\n",
    "        X_ii = eval(dataset_ii)\n",
    "        for jj in range(0,N):\n",
    "            dataset_jj = 'dataset' + str(jj)\n",
    "            X_jj = eval(dataset_jj)\n",
    "            if SimilarityFlag == 0:\n",
    "                SimWithinSite = euclidean_distances(X_ii,X_jj) \n",
    "                Sim[ii,jj] = np.linalg.norm(SimWithinSite,ord = 'fro')/(float(M.shape[0])*float(M.shape[1]))\n",
    "            elif SimilarityFlag == 1:\n",
    "                SimWithinSite = Kern.rbf_kernel(X_ii,X_jj,bw_prob)\n",
    "                Sim[ii,jj] = np.mean(SimWithinSite)\n",
    "\n",
    "    if SimilarityFlag == 2:\n",
    "        SimFinal = np.zeros([N,N])\n",
    "        for ii in range(0,N):\n",
    "            dataset_ii = 'dataset' + str(ii)\n",
    "            for jj in range(0,N):\n",
    "                dataset_jj = 'dataset' + str(jj)\n",
    "                if SimilarityFlag == 0:\n",
    "                    SimFinal[ii,jj] = Sim[ii,jj]\n",
    "                elif SimilarityFlag == 1:\n",
    "                    expVar = Sim[i,i] + Sim[j,j] - 2*Sim[i,j]\n",
    "                    SimFinal[ii,jj] = np.exp(-sim*bwFinal)\n",
    "        Sim = np.copy(SimFinal)\n",
    "    return Sim, datasetTotal\n",
    "\n",
    "\n",
    "def getSimilarityMatrixEra(NormalizationFlag,SimilarityFlag,filepath):\n",
    "    dataset_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for files in glob.glob(cwd + filepath+\"*.csv\"):\n",
    "        dataset_list.append(files)\n",
    "    \n",
    "    N = len(dataset_list)\n",
    "    dataset = genfromtxt(dataset_list[1], delimiter=',')\n",
    "    datasetTotal = np.zeros([1,dataset.shape[1]-2])\n",
    "    for site in range(0,N):\n",
    "    \n",
    "        dataset_file = dataset_list[site]\n",
    "        dataset = genfromtxt(dataset_file, delimiter=',')\n",
    "    \n",
    "        if len(dataset.shape) == 2:\n",
    "            datasetX = dataset[:,:-2]\n",
    "            datasetERA = dataset[:,-1]\n",
    "            datasetID = dataset[:,-2]\n",
    "        else: \n",
    "            dataset = dataset[np.newaxis,:]\n",
    "            datasetX = dataset[:,:-2]\n",
    "            datasetERA = dataset[:,-1]\n",
    "            datasetID = dataset[:,-2]\n",
    "    \n",
    "        if NormalizationFlag == 0:\n",
    "            dataset_normalized = preprocessing.normalize(datasetX, norm='l2')\n",
    "        elif NormalizationFlag == 1:\n",
    "            dataset_normalized = preprocessing.scale(datasetX)\n",
    "        elif NormalizationFlag == 2:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset_normalized = min_max_scaler.fit_transform(datasetX)\n",
    "        \n",
    "        dataset_site = 'dataset' + str(site)\n",
    "        exec (dataset_site + \" = dataset_normalized\")\n",
    "        \n",
    "        dataset_site_era = 'dataset_era' + str(site)\n",
    "        exec (dataset_site_era + \" = datasetERA\" )\n",
    "    \n",
    "        datasetTotal = np.concatenate((datasetTotal,dataset_normalized),axis = 0)\n",
    "    \n",
    "    datasetTotal = datasetTotal[1:,:]\n",
    "    bw_prob = getSigma(datasetTotal,'Scott')\n",
    "    print bw_prob\n",
    "    Sim = np.zeros([N,N])\n",
    "    for ii in range(0,N):\n",
    "        dataset_ii = 'dataset' + str(ii)\n",
    "        X_iiEra = eval('dataset_era' + str(ii))\n",
    "        X_ii = eval(dataset_ii)\n",
    "        for jj in range(0,N):\n",
    "            dataset_jj = 'dataset' + str(jj)\n",
    "            X_jjEra = eval('dataset_era' + str(jj))\n",
    "            X_jj = eval(dataset_jj)\n",
    "            if SimilarityFlag == 0:\n",
    "                SimWithinSite = euclidean_distances(X_ii,X_jj) \n",
    "                Sim[ii,jj] = np.linalg.norm(SimWithinSite,ord = 'fro')/(float(M.shape[0])*float(M.shape[1]))\n",
    "            elif SimilarityFlag == 1:\n",
    "                SimWithinSite = Kern.rbf_kernel(X_ii,X_jj,bw_prob)\n",
    "                EraSim = np.zeros([X_ii.shape[0],X_jj.shape[0]])\n",
    "                for pp in range(0,X_ii.shape[0]):\n",
    "                    for qq in range(0,X_jj.shape[0]):\n",
    "                        EraSim[pp,qq] = np.sign(X_iiEra[pp] - X_jjEra[qq])\n",
    "                \n",
    "                \n",
    "                Sim[ii,jj] = np.mean(SimWithinSite*EraSim)\n",
    "    return Sim\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 28)\n",
      "[ 1.28098037 -0.79844373  0.85467077  0.36726009 -0.79793272  0.92298246\n",
      "  0.14888045  0.24525683  0.31284791  0.76182416  1.99694921  0.51786186\n",
      "  0.57705174  0.50801392  1.78053587  0.7952699   0.12932222  1.33081139\n",
      " -0.2258994   0.13236769 -0.04926247  1.49095513  0.66339908  0.77394755\n",
      " -0.55702339 -0.3740366  -0.01100073]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.\n",
      "   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.\n",
      "   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.\n",
      "   2.   2.   2.   2.   2.   2.   2.   2.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   3.   3.   3.   3.   3.   3.   3.   3.   4.   4.   4.   4.   4.   4.   4.\n",
      "   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.\n",
      "   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.\n",
      "   4.   4.   4.   4.   4.   4.   4.   4.   4.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.\n",
      "   6.   6.   6.   6.   6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.\n",
      "   7.   7.   7.   7.   7.   7.   8.   8.   8.   8.   8.   8.   8.   8.   8.\n",
      "   8.   8.   8.   8.   8.   8.   8.   8.   8.   8.   8.   9.   9.   9.   9.\n",
      "   9.   9.   9.   9.   9.   9.   9.   9.   9.   9.   9.   9.   9.   9.   9.\n",
      "   9.   9.   9.   9.   9.   9.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n",
      "  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n",
      "  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n",
      "  10.  10.  10.  10.  10.  10.  10.  10.  10.  11.  11.  11.  11.  11.  11.\n",
      "  11.  11.  11.  11.  11.  11.  11.  11.  11.  11.  11.  11.  11.  11.  12.\n",
      "  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.\n",
      "  12.  12.  12.  12.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.\n",
      "  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.\n",
      "  13.  13.  13.  13.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.\n",
      "  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.\n",
      "  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.\n",
      "  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.  14.\n",
      "  14.  14.  14.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.\n",
      "  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.\n",
      "  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.\n",
      "  15.  15.  15.  15.  15.  15.  15.  15.  15.  16.  16.  16.  16.  16.  16.\n",
      "  16.  16.  16.  16.  16.  16.  16.  16.  16.  16.  16.  16.  17.  17.  17.\n",
      "  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.\n",
      "  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.  17.\n",
      "  17.  17.  17.  17.  17.  17.  17.  18.  18.  18.  18.  18.  18.  18.  18.\n",
      "  18.  18.  18.  18.  18.  18.  18.  18.  18.  18.  18.  18.  19.  19.  19.\n",
      "  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.\n",
      "  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.\n",
      "  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.  19.]\n"
     ]
    }
   ],
   "source": [
    "#from SimilaritMatrixCreation import *\n",
    "#from plot_network import *\n",
    "import numpy as np\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "filepath = \"/Site_specific_data/\"\n",
    "\n",
    "\n",
    "NormalizationFlag = 1 # 0 is normalizing by norm, 1 is scale it, 2 is make it in between [0,1]\n",
    "SimilarityFlag = 1 #0 if euclidean DISTANCE (note distance and similarity), 1 if RKHS (mean embedding) with linear kernel, 2 if RKHS with Gaussian kernel\n",
    "    \n",
    "\n",
    "datasetTotal, labelTotal = datasetLabels(NormalizationFlag,SimilarityFlag,filepath)\n",
    "\n",
    "print datasetTotal[0,:]\n",
    "\n",
    "print labelTotal\n",
    "#plot_network(abs(SimEra),plot_type='plain',label_type='numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(822,27)\n",
    "y = np.ones(822,1)\n",
    "y[0:400] = -1\n",
    "\n",
    "\n",
    "Xtrain = "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import sklearn.metrics.pairwise as Kern\n",
    "from sklearn import preprocessing \n",
    "import glob\n",
    "import os\n",
    "\n",
    "def datasetLabels(NormalizationFlag,SimilarityFlag,filepath):\n",
    "    dataset_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for files in glob.glob(cwd +filepath+ \"*.csv\"):\n",
    "        dataset_list.append(files)\n",
    "    \n",
    "    N = len(dataset_list)\n",
    "    dataset = genfromtxt(dataset_list[1], delimiter=',')\n",
    "    datasetTotal = np.zeros([1,dataset.shape[1]-1])\n",
    "    print dataset.shape\n",
    "    labelTotal = np.zeros([1])\n",
    "    for site in range(0,N):\n",
    "    \n",
    "        dataset_file = dataset_list[site]\n",
    "        dataset = genfromtxt(dataset_file, delimiter=',')\n",
    "    \n",
    "        if len(dataset.shape) == 2:\n",
    "            datasetX = dataset[:,:-1]\n",
    "            datasetID = dataset[:,-1]\n",
    "        else: \n",
    "            dataset = dataset[np.newaxis,:]\n",
    "            datasetX = dataset[:,:-1]\n",
    "            datasetID = dataset[:,-1]\n",
    "    \n",
    "        if NormalizationFlag == 0:\n",
    "            dataset_normalized = preprocessing.normalize(datasetX, norm='l2')\n",
    "        elif NormalizationFlag == 1:\n",
    "            dataset_normalized = preprocessing.scale(datasetX)\n",
    "        elif NormalizationFlag == 2:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset_normalized = min_max_scaler.fit_transform(datasetX)\n",
    "        \n",
    "        dataset_site = 'dataset' + str(site)\n",
    "        exec (dataset_site + \" = dataset_normalized\")\n",
    "        datasetTotal = np.concatenate((datasetTotal,dataset_normalized),axis = 0)\n",
    "        labels = site*np.ones([dataset_normalized.shape[0]])\n",
    "        labelTotal = np.concatenate((labelTotal,labels),axis = 0)\n",
    "        \n",
    "    datasetTotal = datasetTotal[1:,:]\n",
    "    labelTotal = labelTotal[1:]\n",
    "    \n",
    "    return datasetTotal, labelTotal\n",
    "\n",
    "filepath = \"/Site_specific_data/\"\n",
    "\n",
    "\n",
    "NormalizationFlag = 1 # 0 is normalizing by norm, 1 is scale it, 2 is make it in between [0,1]\n",
    "SimilarityFlag = 1 #0 if euclidean DISTANCE (note distance and similarity), 1 if RKHS (mean embedding) with linear kernel, 2 if RKHS with Gaussian kernel\n",
    "    \n",
    "\n",
    "datasetTotal, labelTotal = datasetLabels(NormalizationFlag,SimilarityFlag,filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for in range(0,)\n",
    "a = random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "20\n",
      "25\n",
      "38\n",
      "100\n",
      "20\n",
      "20\n",
      "20\n",
      "41\n",
      "48\n",
      "59\n",
      "40\n",
      "20\n",
      "68\n",
      "43\n",
      "28\n",
      "40\n",
      "51\n",
      "18\n",
      "30\n",
      "[ 5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "  4.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  4.  4.\n",
      "  4.  4.  4.  4.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "  4.  4.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  2.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  4.  4.  4.  4.  4.  4.  4.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  4.  4.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.]\n",
      "(740, 27)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datasetLabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fcfea32c66e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[0maccu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetLabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNormalizationFlag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSimilarityFlag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m     \u001b[0mnumInstances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumInstances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasetLabels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import sklearn.metrics.pairwise as Kern\n",
    "from sklearn import preprocessing \n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def datasetLabelsEra(NormalizationFlag,SimilarityFlag,filepath):\n",
    "    dataset_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for files in glob.glob(cwd + filepath+\"*.csv\"):\n",
    "        dataset_list.append(files)\n",
    "\n",
    "    N = len(dataset_list)\n",
    "    dataset = genfromtxt(dataset_list[1], delimiter=',')\n",
    "    datasetTotal = np.zeros([1,dataset.shape[1]-2])\n",
    "    labelTotal = np.zeros([1])\n",
    "    for site in range(0,N):\n",
    "\n",
    "        dataset_file = dataset_list[site]\n",
    "        dataset = genfromtxt(dataset_file, delimiter=',')\n",
    "\n",
    "        if len(dataset.shape) == 2:\n",
    "            datasetX = dataset[:,:-2]\n",
    "            datasetERA = dataset[:,-1]\n",
    "            datasetID = dataset[:,-2]\n",
    "        else:\n",
    "            dataset = dataset[np.newaxis,:]\n",
    "            datasetX = dataset[:,:-2]\n",
    "            datasetERA = dataset[:,-1]\n",
    "            datasetID = dataset[:,-2]\n",
    "\n",
    "        if NormalizationFlag == 0:\n",
    "            dataset_normalized = preprocessing.normalize(datasetX, norm='l2')\n",
    "        elif NormalizationFlag == 1:\n",
    "            dataset_normalized = preprocessing.scale(datasetX)\n",
    "        elif NormalizationFlag == 2:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset_normalized = min_max_scaler.fit_transform(datasetX)\n",
    "\n",
    "        dataset_site = 'dataset' + str(site)\n",
    "        exec (dataset_site + \" = dataset_normalized\")\n",
    "        datasetEraOld = np.copy(datasetERA)\n",
    "        print datasetERA.shape[0]\n",
    "        for ii in range(0,datasetERA.shape[0]):\n",
    "            if datasetEraOld[ii] <= -1400:\n",
    "                datasetERA[ii] = 1\n",
    "            elif datasetEraOld[ii] <=-1300 and datasetEraOld[ii] >= -1399:\n",
    "                datasetERA[ii] = 2\n",
    "            elif datasetEraOld[ii] <= -1200 and datasetEraOld[ii] >= -1299:\n",
    "                datasetERA[ii] = 3\n",
    "            elif datasetEraOld[ii] <= -1050 and datasetEraOld[ii] >= -1199:\n",
    "                datasetERA[ii] = 4\n",
    "            elif datasetEraOld[ii] >= -1050:\n",
    "                datasetERA[ii] = 5\n",
    "        dataset_site_era = 'dataset_era' + str(site)\n",
    "        exec (dataset_site_era + \" = datasetERA\" )\n",
    "\n",
    "        datasetTotal = np.concatenate((datasetTotal,dataset_normalized),axis = 0)\n",
    "        #labels = site*np.ones([dataset_normalized.shape[0]])\n",
    "        labelTotal = np.concatenate((labelTotal,datasetERA),axis = 0)\n",
    "\n",
    "    datasetTotal = datasetTotal[1:,:]\n",
    "    labelTotal = labelTotal[1:]\n",
    "    \n",
    "    \n",
    "    return datasetTotal, labelTotal\n",
    "\n",
    "\n",
    "filepath = \"/Site_specific_data_with_ERA/\"\n",
    "\n",
    "NormalizationFlag = 1 # 0 is normalizing by norm, 1 is scale it, 2 is make it in between [0,1]\n",
    "SimilarityFlag = 1 #0 if euclidean DISTANCE (note distance and similarity), 1 if RKHS (mean embedding) with linear kernel, 2 if RKHS with Gaussian kernel\n",
    "\n",
    "X, Y = datasetLabelsEra(NormalizationFlag,SimilarityFlag,filepath)\n",
    "print Y\n",
    "print X.shape\n",
    "\n",
    "\n",
    "T = 1\n",
    "numTrain = 500\n",
    "accu = np.zeros([T])\n",
    "for tt in range(0,T):\n",
    "    X, Y = datasetLabelsEra(NormalizationFlag,SimilarityFlag,filepath)\n",
    "    numInstances = X.shape[0]\n",
    "    ind = np.random.permutation(numInstances)\n",
    "    XTrain = X[ind[0:numTrain],:]\n",
    "    YTrain = Y[ind[0:numTrain]]\n",
    "    XTest = X[ind[numTrain:],:]\n",
    "    YTest = Y[ind[numTrain:]]\n",
    "\n",
    "    #print Y\n",
    "    clf = SVC()\n",
    "    clf.fit(XTrain, YTrain)\n",
    "    YPred = clf.predict(XTest)\n",
    "    print \"size of test data\"\n",
    "    print YPred.shape, YTest.shape\n",
    "    print \"No Cross Validation Accuracy is\"\n",
    "    print np.sum(YPred == YTest)/float(numInstances - numTrain)\n",
    "\n",
    "    #plt.plot(YPred)\n",
    "    #plt.plot(Y)\n",
    "    #plt.show()\n",
    "    #print np.sum(YTest==0), np.sum(YTest==1),np.sum(YTest==2),np.sum(YTest==3),np.sum(YTest==4)\n",
    "    #print np.sum(YPred==0), np.sum(YPred==1),np.sum(YPred==2),np.sum(YPred==3),np.sum(YPred==4)\n",
    "\n",
    "    #k_fold = KFold(n_splits = 5)\n",
    "    #[svc.fit(XTrain[train],yTrain[train]).score(XTrain[test],yTrain[test])for train,test in k_fold.split(XTrain)]\n",
    "\n",
    "    CGrid = np.logspace(-4,1,20)\n",
    "    GammaGrid = np.logspace(-3,3,20)\n",
    "    clf_cv = GridSearchCV(estimator = SVC(kernel = 'rbf'), param_grid = dict(C = CGrid,gamma = GammaGrid),cv = 5)\n",
    "    clf_cv.fit(XTrain,YTrain)\n",
    "    print \"Best parameters\"\n",
    "    print clf_cv.best_estimator_.C,clf_cv.best_estimator_.gamma\n",
    "\n",
    "    clf = SVC(C = clf_cv.best_estimator_.C, gamma = clf_cv.best_estimator_.gamma,kernel = 'rbf')\n",
    "    clf.fit(XTrain,YTrain)\n",
    "    YPred = clf.predict(XTest)\n",
    "    print \"Size of test Data\"\n",
    "    print YPred.shape,YTest.shape,numInstances - numTrain\n",
    "    print \"Train Data Distribution according to classes are\"\n",
    "    TrainHist =  np.histogram(YTrain,20)[0]/float(numTrain)\n",
    "    print TrainHist, np.sum(TrainHist)\n",
    "    print \"Test Data distribution according to classes are\"\n",
    "    TestHist =  np.histogram(YTest,20)[0]/float(numInstances-numTrain)\n",
    "    print TestHist, np.sum(TestHist)\n",
    "    print \"Accuracy with Cross Validation is: \"\n",
    "    accu[tt] = np.sum(YPred == YTest)/float(numInstances - numTrain)\n",
    "    print np.sum(YPred == YTest)/float(numInstances -numTrain)\n",
    "\n",
    "print \"Average accuracy is: \" + str(np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
